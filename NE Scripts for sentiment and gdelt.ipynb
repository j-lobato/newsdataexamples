{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment extraction scripts\n",
    "\n",
    "This notebook contains scripts for calculating the sentiment score for raw text. The text can be extracted from various sources, such as scraping the GDELT links, raw txts from Factiva or tweets from twitter. The scripts here assume that the data is already in a raw text format, thus it is required to load the data in some other function before calling the sentiment methods presented here.\n",
    "\n",
    "The method presented in this notebook is rather simple yet efficient. It is based on having sets of keywords for negative and positive sentiment and calculating the frequency of those words in the lemmatized unigrams (eg. bag of words)\n",
    "\n",
    "**Advise for calculating a time series of sentiment**\n",
    "* Calculate each time window separately and save the intermediate results in a temporary file. This helps to avoid losing computation time in case there are crashes or other limitations\n",
    "* Use Unix time epochs for each day or other unit of time\n",
    "* CSV and JSON are good formats for storing the data\n",
    "* Using notebooks for the analysis is recommended, but for pure calculations the script format is more efficient\n",
    "\n",
    "An example for loading GDELT data in memory and calculating the sentiment is presented at the end of this notebook.\n",
    "\n",
    "Notice that these scripts are for advice and inspiration, running them needs modifications and adjustments regarding the system you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tuomastakko/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import string\n",
    "import time\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the sentiment dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.field_size_limit(sys.maxsize)\n",
    "negative_words = ('jitter','threatening','distrusted','jeopardized','jitters','hurdles','fears','feared','traumatic','fail','erodes','uneasy','distressed','unease','disquieted','perils','traumas','alarm','distrusting','doubtable','terrors','worries','panics','eroding','terrifying','doubt','traumatised','panic','imperils','mistrusts','failings','nervousness','conflicted','reject','doubting','fearing','dreads','distrust','disquiet','questioned')\n",
    "positive_words=('excited','incredible','ideal','attract','tremendous','satisfactorily','brilliant','meritorious','superbly','satisfied','perfect','win','amazes','energizing','gush','wonderful','attracts','enthusiastically','exceptionally','encouraged','excels','impressively','encouraging','impress','favoured','enjoy','pleasures','positive','unique','impressed','enhances','delighted','energise','spectacular','enjoyed','enthusiastic','inspiration','galvanized','amaze','excelling')\n",
    "ecount = 0 \n",
    "tte=0\n",
    "acount = 0 \n",
    "tta=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Scraping tool for getting synonyms from Merriam Webster\n",
    "\n",
    "'''\n",
    "def scrapewebster(word):\n",
    "    # specify which URL/web page we are going to be scraping\n",
    "    url = \"https://www.merriam-webster.com/thesaurus/\"+(word.replace(' ','_'))\n",
    "\n",
    "    page = urllib.request.urlopen(url)\n",
    "\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "\n",
    "    infobox = []\n",
    "    for row in soup.findAll('div', class_='thes-list-content synonyms_list'):\n",
    "        keys=row.findAll('li')\n",
    "        k = None\n",
    "        for i in keys:\n",
    "            k = i.find(text=True)\n",
    "            tmp = k.replace('(','').replace(')','')\n",
    "            if len(tmp)>1:\n",
    "                infobox.append(tmp)\n",
    "\n",
    "    return infobox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapethesaur(entity):\n",
    "    # specify which URL/web page we are going to be scraping\n",
    "    url = \"https://www.thesaurus.com/browse/\"+(entity.replace(' ','_'))\n",
    "\n",
    "    page = urllib.request.urlopen(url)\n",
    "\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "\n",
    "    infobox = []\n",
    "    \n",
    "    interpret = soup.findAll('div', class_='css-kv266z e1qo4u830')[0]\n",
    "    \n",
    "    #for interpret in interprets:\n",
    "    #    print(interpret)\n",
    "    for row in interpret.findAll('ul', class_='css-1ytlws2 et6tpn80'):\n",
    "        keys=row.findAll('li')\n",
    "        k = None\n",
    "        for i in keys:\n",
    "            k = i.find(text=True)\n",
    "            tmp = k.replace('(','').replace(')','')\n",
    "            if len(tmp)>1:\n",
    "                infobox.append(tmp)\n",
    "            #for j in k.find(text=True):\n",
    "            #    infobox.append(j)\n",
    "    return infobox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function for calculating the occurrence of positive and negative words in the string parameter body.\n",
    "\n",
    "Note that this does not use lemmatization and is not as accurate.\n",
    "\n",
    "'''\n",
    "def body_keyword_count(body, key_positive, key_negative):\n",
    "    counter_p = 0 \n",
    "    counter_n = 0\n",
    "    for word in body.split():\n",
    "        if word in key_positive:\n",
    "            counter_p += 1\n",
    "        if word in key_negative:\n",
    "            counter_n += 1\n",
    "    \n",
    "    return (len(body.split()), counter_p, counter_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function for calculating the occurrence of positive and negative words in the string parameter body.\n",
    "\n",
    "Uses lemmatization for the dictionary words and compares the lemmas to the words in the text.\n",
    "\n",
    "'''\n",
    "def body_keyword_count_lemma(body, key_positive, key_negative):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    counter_p = 0 \n",
    "    counter_n = 0\n",
    "    for word in body.split():\n",
    "        for key_p in positive_words:\n",
    "            if lemmatizer.lemmatize(key_p) in word:\n",
    "                counter_p += 1\n",
    "                break\n",
    "        for key_n in negative_words:\n",
    "            if lemmatizer.lemmatize(key_n) in word:\n",
    "                counter_n += 1\n",
    "                break        \n",
    "        \n",
    "    return (len(body.split()), counter_p, counter_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b0cb3f0f6b7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minfobox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtranslate_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trust'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-b0cb3f0f6b7f>\u001b[0m in \u001b[0;36mtranslate_token\u001b[0;34m(word, from_lang, to_lang)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_lang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://translate.google.com/#view=home&op=translate&sl='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfrom_lang\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'&tl='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mto_lang\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'&text='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lxml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 641\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Scrape Google Translate\n",
    "'''\n",
    "\n",
    "def translate_token(word, from_lang, to_lang):\n",
    "    url = 'https://translate.google.com/#view=home&op=translate&sl='+from_lang+'&tl='+to_lang+'&text='+word\n",
    "    page = urllib.request.urlopen(url)\n",
    "\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "\n",
    "    infobox = []\n",
    "    for row in soup.findAll('div', class_='result-shield-container tlid-copy-target'):\n",
    "        keys=row.findAll('span')\n",
    "        k = None\n",
    "        for i in keys:\n",
    "            k = i.find(text=True)\n",
    "            tmp = k.replace('(','').replace(')','')\n",
    "            if len(tmp)>1:\n",
    "                infobox.append(tmp)\n",
    "    return infobox\n",
    "\n",
    "translate_token('trust', 'en', 'fi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use different lexicons for different sentiments?\n",
    "\n",
    "Using the functions described above, one can calculate the occurrence of the keyword counts for each article.\n",
    "\n",
    "The process is the following:\n",
    "\n",
    "1. Decide which words and their synonyms you want to include to the BOW.\n",
    "2. Use scrapewebster and scrapethesaur for getting the sets of words (with antonyms as well) ie. scrapethesaur('trust') scrapethesaur('distrust')\n",
    "1. Iterate through your local files containing the text\n",
    "4. For each text body, run the body_keyword_count_lemma(text, lex1, lex2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Example\n",
    "\n",
    "'''\n",
    "\n",
    "data = ['The authors show that sentiments from newspaper articles can explain and predict movements in the term structure of U.S. government bonds. This effect is stronger at the short end of the curve, coinciding with greater volatility and investors need to continually reassess the Feds reaction function. Facing such uncertainty, market participants rely on news and sentiment as a central element in their decision-making process. Considering this dependence, the authors propose a new yield curve factor—news sentiment—that is distinct from the 3 established yield curve factors (level, slope, and curvature) as well as from fundamental macroeconomic variables.']\n",
    "trust = scrapethesaur('trust')\n",
    "distrust = scrapethesaur('distrust')\n",
    "\n",
    "for body in data:\n",
    "    print(body_keyword_count_lemma(body, trust, distrust))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the analysis\n",
    "\n",
    "For running the analysis one should have the textual data in a readable format (ie. cleaned from markup tags etc).\n",
    "The following is a simple example on how one would run the script if the text files were stored in a local folder and the filename contained the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelocation = 'location/of/files/'\n",
    "dailysentiments = {}\n",
    "for textfile in glob.glob(filelocation+'*.txt'):\n",
    "    filedate = textfile.split('/')[-1][:-4]\n",
    "    with open(textfile, 'r') as readfile:\n",
    "        textdata = readfile.read()\n",
    "    dailysentiments[filedate] = body_keywork_count_lemma(textdata,positive_words, negative_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gdelt example\n",
    "\n",
    "This example from the last semester's project course shows a way of filtering relevant GDELT news using the precalculated tags. One can use the sentiment (V2Tone) in parallel with the manually calculated bag of words sentiment. The topics in this example include financial institutions, but changing the topics and entities listed changes the results.\n",
    "\n",
    "This particular example uses the csv format. If the json format is more familiar to you, check the other notebook provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the GKG dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml.html as lh\n",
    "import sys \n",
    "import os.path\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import glob\n",
    "import operator\n",
    "\n",
    "'''\n",
    "Here we construct the system for downloading the csv files for Gdelt GKG\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "infilecounter = 0\n",
    "outfilecounter = 0\n",
    " \n",
    "\n",
    "gdelt_base_url = 'http://data.gdeltproject.org/gkg/' \n",
    "\n",
    "# get the list of all the links on the gdelt file page\n",
    "page = requests.get(gdelt_base_url+'index.html')\n",
    "doc = lh.fromstring(page.content)\n",
    "link_list = doc.xpath(\"//*/ul/li/a/@href\")\n",
    "\n",
    "# separate out those links that begin with four digits \n",
    "file_list = [x for x in link_list if str.isdigit(x[0:4]) and x[9:17] != 'gkgcount']\n",
    "\n",
    "# uncomment this line to run the full crawl\n",
    "file_list = file_list[0:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = 'yourpath' # Change this path according to the system you are using!!\n",
    "\n",
    "debug_condition_3 = False\n",
    "\n",
    "'''\n",
    "The following functions test if the topics or entities are present in the gkg file format,\n",
    "eg. filters the correct news pieces.\n",
    "\n",
    "'''\n",
    "# it returns True if at least one key is in list_of_items\n",
    "def themesInEntry(list_of_keys, list_of_items):\n",
    "    for key in list_of_keys:\n",
    "        if key in list_of_items:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def organizationInEntry(orgs, column):\n",
    "    for entry in column:\n",
    "        if entry in orgs:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "file_is_empty = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips_country_code = 'US'\n",
    "fips_country_code_hash = '#US#'\n",
    "themes = ['WB_1234_BANKING_INSTITUTIONS','WB_1236_COMMERCIAL_BANKING','WB_1256_CREDIT_UNIONS','ECON_DEBT','ECON_STOCK_MARKET','WB_1234_BANKING_INSTITUTIONS','WB_1920_FINANCIAL_SECTOR_DEVELOPMENT','ECON_CENTRALBANK','WB_318_FINANCIAL_ARCHITECTURE_AND_BANKING','WB_332_CAPITAL_MARKETS','WB_611_PENSION_FUNDS','WB_971_BANKING_REGULATION']\n",
    "organization = ['1347 property insurance', \t'acmat corporation', \t'aflac', \t'alleghany', \t'allstate', \t'ambac financial group', \t'american financial group', \t'american international group', \t'amerisafe', \t'arthur j gallagher', \t'assurant', \t'atlas financial holdings', \t'berkshire hathaway', \t'blue water global', \t'brown brown', \t'brp group', \t'cincinnati financial corporation', \t'cna financial corporation', \t'cno financial group', \t'conifer holdings', \t'corvel corporation', \t'crawford company', \t'donegal group', \t'ehealth', \t'employers holdings', \t'equitable holdings', \t'erie indemnity company', \t'fednat holding co', \t'fidelity national financial', \t'first acceptance corporation', \t'first american financial corporation', \t'gainsco', \t'goosehead insurance', \t'grand havana', \t'hallmark financial services', \t'hanover insurance group', \t'hartford financial services group', \t'hci group', \t'health insurance innovations', \t'heritage insurance holdings', \t'hilltop holdings', \t'horace mann educators corporation', \t'huize holding', \t'icc holdings', \t'inspro technologies corporation', \t'investors title company', \t'kemper corporation', \t'kingstone companies', \t'kinsale capital group', \t'loews corporation', \t'markel corporation', \t'marsh mclennan companies', \t'mbia', \t'mercury general corporation', \t'mgic investment corporation', \t'national general holdings corp', \t'ni holdings', \t'nmi holdings', \t'old republic international corporation', \t'pacific ventures groupinc', \t'palomar holdings inc', \t'pmi group inc', \t'positive physicians holdingsinc', \t'principal financial group', \t'proassurance corporation', \t'progressive corp', \t'prosight global inc', \t'protective insurance corp', \t'qbe insurance group limited - adr', \t'radian group', \t'reinsurance group of america inc', \t'rli corp', \t'safety insurance group inc', \t'selective insurance group', \t'state auto financial corp', \t'stewart information services corporation', \t'sundance strategies', \t'travelers companies', \t'triad guaranty', \t'unico american corporation', \t'united fire casualty co', \t'united insurance holdings corp', \t'universal insurance holdings', \t'unum group', \t'voya financial', \t'w r berkley corp', \t'american equity investment life holding', \t'american national insurance', \t'atlantic american corporation ', \t'brighthouse financial', \t'citizens financial corp', \t'citizens inc', \t'emergent capital inc ', \t'fbl financial group', \t'federal life group', \t'galaxy next generation', \t'genworth financial', \t'globe life inc', \t'gwg holdings', \t'independence holding company ', \t'kansas city life insurance company ', \t'lincoln national', \t'metlife', \t'national security group', \t'national western life group', \t'primerica', \t'prudential', \t'sanlam limited', \t'utg Inc', \t'vericity']\n",
    "\n",
    "for compressed_file in file_list[infilecounter:]:\n",
    "    print(compressed_file)\n",
    "    # if we dont have the compressed file stored locally, go get it. Keep trying if necessary.\n",
    "    while not os.path.isfile(local_path+compressed_file):\n",
    "        urllib.request.urlretrieve(url=gdelt_base_url+compressed_file,   filename=local_path+compressed_file)\n",
    "    # extract the contents of the compressed file to a temporary directory\n",
    "    print('extracting')\n",
    "    z = zipfile.ZipFile(file=local_path+compressed_file, mode='r')\n",
    "    z.extractall(path=local_path+'tmp/')\n",
    "    # parse each of the csv files in the working directory,\n",
    "    print('parsing')\n",
    "    for infile_name in glob.glob(local_path+'tmp/*'):\n",
    "        # WE INSERTED THE FOLLOWING LINE\n",
    "        if not os.path.exists(local_path+'country/'):\n",
    "            os.mkdir(local_path+'country/')\n",
    "        outfile_name = local_path+'country/'+fips_country_code+'%04i.csv'%outfilecounter\n",
    "        # open the infile and outfile\n",
    "        with open(infile_name, mode='r', encoding=\"utf8\") as infile, open(outfile_name, mode='w', encoding=\"utf8\") as outfile:\n",
    "            for line in infile:\n",
    "                if line[0:4] == 'DATE':\n",
    "                    continue\n",
    "                # extract lines with our interest country code\n",
    "                #print(line) \n",
    "                #print(\"lenght of line split is {}\".format(len(line.split('\\t'))))\n",
    "                \n",
    "                #for i in range(len(line.split('\\t'))):\n",
    "                #   print(\"{} is : {}\".format(i, line.split('\\t')[i]))\n",
    "                #sys.exit()\n",
    "                \n",
    "                \n",
    "                \n",
    "                if fips_country_code_hash in line.split('\\t')[4] and themesInEntry(themes, line.split('\\t')[3].split(';')) and themesInEntry(organization, line.split('\\t')[6].split(';')): \n",
    "                    if debug_condition_3 is True:\n",
    "                        if themesInEntry(organization, line.split('\\t')[6].split(';')):\n",
    "                            for key in organization:\n",
    "                                for item in line.split('\\t')[6].split(';'):\n",
    "                                    if key in item:\n",
    "                                        print(\"'{}' IN '{}'\".format(key, item))\t\t\t\n",
    "                        \n",
    "                    \n",
    "            \n",
    "                    file_is_empty  = False\n",
    "                    outfile.write(line)\n",
    "            outfilecounter +=1\n",
    "        # delete the temporary file\n",
    "        os.remove(infile_name)\n",
    "        z.close()\n",
    "        os.remove(local_path+compressed_file)\n",
    "    if file_is_empty == True:\n",
    "        if merge_results == True:\n",
    "            os.remove(outfile_name)\n",
    "    infilecounter +=1\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping a sample of the articles using BeautifulSoup\n",
    "\n",
    "Saving the raw texts into separate files is once again recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import time, os, sys\n",
    "\n",
    "\n",
    "# change these two lines to scrape different files\n",
    "folder = 'mutuals'\n",
    "input_csv = 'full_shuffled.csv'\n",
    "\n",
    "local_path = 'path to your dir'\n",
    "\n",
    "'''\n",
    "This script used only a small sample of the available data. \n",
    "Remove these target variables if you want the full set of data.\n",
    "'''\n",
    "start = 0\n",
    "end = 30000\n",
    "target_lines = 6000\n",
    "\n",
    "output_csv = 'scraped_articles_'+str(target_lines)+'.csv'\n",
    "infile_name = local_path+input_csv\n",
    "outfile_name = local_path+output_csv\n",
    "\n",
    "write_on_file = True\n",
    "# open the collated csv\n",
    "infile = open(infile_name, mode='r', encoding=\"utf8\")\n",
    "\n",
    "\n",
    "# start a foor loop that reads one line at a time\n",
    "\n",
    "counter = 0\n",
    "\n",
    "if write_on_file:            \n",
    "    outfile = open(outfile_name, mode ='w+', encoding=\"utf8\")\n",
    "    \n",
    "if os.path.isdir(outfile_name):\n",
    "    print(\"ERROR file is not there\")\n",
    "    sys.exit()\n",
    "    \n",
    "    \n",
    "counter_written = 0\n",
    "for line in infile:\n",
    "    counter += 1\n",
    "    # for each line get the url entry\n",
    "    url = line.split('\\t')[10]\n",
    "    try:\n",
    "        page = urlopen(url, timeout=10)\n",
    "    except:\n",
    "        print(\"Link {} FAILED\".format(counter))\n",
    "        continue\n",
    "\n",
    "    print(\"Link {} OK\".format(counter))\n",
    "    # parse the html using beautiful soup and store in variable `soup`\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "\n",
    "    # extract the title \n",
    "    try:\n",
    "        title = soup.title.string\n",
    "    except:\n",
    "        title = \"NoTitle\"\n",
    "        \n",
    "    if title is None:\n",
    "        title = \"NoTitle\"\n",
    "        \n",
    "    # extract the body of the entry \n",
    "    # Take out the <div> of name and get its value\n",
    "    #content = soup.find('div', {\"class\": \"story-body sp-story-body gel-body-copy\"})    \n",
    "    content = soup.body\n",
    "\n",
    "    if content is not None:\n",
    "        print(\"Link {} has Content\".format(counter))\n",
    "        article = ''\n",
    "        for i in content.findAll('p'):\n",
    "            new_text = i.text.replace('\"',' ')\n",
    "            article = article + '~' +  new_text\n",
    "        # save the title and body on file \n",
    "        # Saving the scraped text\n",
    "        if len(article) > 0:\n",
    "            print(\"Link {} has Text [{}] \\n\".format(counter, len(content.findAll('p'))))\n",
    "            #print(line.split('\\t')[0]+'\\t'+url+'\\t'+title+'\\t'+article+'\\n')\n",
    "            #prepare strings for writing on file\n",
    "            date = line.split('\\t')[0]\n",
    "            date = date.replace('\\n',' ') \n",
    "            \n",
    "            #url = str(url)\n",
    "            url = url.replace('\\n',' ')\n",
    "            url = url.replace('\\t',' ')\n",
    "            \n",
    "            #title = str(title)\n",
    "            title = title.replace('\\n',' ')\n",
    "            title.replace('\\t',' ')\n",
    "            \n",
    "            #article = str(article)\n",
    "            article = article.replace('\\n',' ')\n",
    "            article.replace('\\t','~')\n",
    "            delimiter = '\\t'\n",
    "            #print(\"{}  {} {} {} \\n\".format(counter, date, title, article[:10]))\n",
    "            string = folder+delimiter+date+delimiter+url+delimiter+title+delimiter+article+'\\n'\n",
    "            #print(string)\n",
    "            if write_on_file:            \n",
    "                counter_written += 1\n",
    "                written_bites = outfile.write(string)\n",
    "                print(\"counter = {} written_bites = {}, url = {}\".format(counter_written, written_bites, url))\n",
    "                \n",
    "                if counter_written == target_lines:\n",
    "                    break      \n",
    "        #sys.exit()\n",
    "print(\"closing file\")\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the sentiment for the downloaded GDELT files\n",
    "\n",
    "The following part produces the output for the downloaded csv files. Recommendation is to split the work into smaller subsets as crashes or errors might result in loss of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'Sentiment'\n",
    "input_csv = 'input.csv'\n",
    "output_csv = \"result.csv\"\n",
    "\n",
    "local_path = 'yourpath/'\n",
    "\n",
    "input_file_path = local_path + input_csv\n",
    "output_file_path = local_path + output_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter  = '\\t'\n",
    "\n",
    "with open(input_file_path,\"r\",encoding=\"utf8\") as inputfile, open(output_file_path,'w',encoding=\"utf8\")as outputfile: #copy to a new file\n",
    "    counter_row = 0\n",
    "    for row in inputfile:\n",
    "        row.lower()\n",
    "        \n",
    "        \n",
    "        row_split = row.split('\\t')\n",
    "\n",
    "        article_type = row_split[0]\n",
    "        date = row_split[1]\n",
    "        url = row_split[2]\n",
    "        title = row_split[3]\n",
    "        body = row_split[4]\n",
    "        body.translate(str.maketrans('','',string.punctuation))\n",
    "        #print(row_split[:3])\n",
    "    \n",
    "        body = body.replace('~', ' ')\n",
    "        \n",
    "        # 1-to-1 match\n",
    "        #num_words, p_count, n_count  = body_keyword_count(body, positive_words, negative_words)\n",
    "\n",
    "        # lemmatized match\n",
    "        num_words, p_count, n_count  = body_keyword_count_lemma(body, positive_words, negative_words)\n",
    "        \n",
    "        \n",
    "        #time.sleep(1)\n",
    "        print(num_words, p_count, n_count)\n",
    "        \n",
    "\n",
    "        line = article_type + delimiter + date + delimiter + str(num_words) + delimiter + str(p_count) + delimiter + str(n_count) + delimiter + url + delimiter + title + '\\n'\n",
    "        \n",
    "        written_bites = outputfile.write(line)\n",
    "        #print(line)\n",
    "        counter_row += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
