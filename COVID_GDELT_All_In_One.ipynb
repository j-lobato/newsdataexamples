{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COVID GDELT All In One",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBv-gOFl-98k",
        "outputId": "bfa312cb-1f34-4555-fb48-8dc16a5f8bad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from io import BytesIO\n",
        "from io import TextIOWrapper\n",
        "from zipfile import ZipFile\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "import sys\n",
        "import csv\n",
        "import os\n",
        "import string\n",
        "import time\n",
        "import nltk\n",
        "import requests\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# or: requests.get(url).content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JAqDNjjCBMr"
      },
      "source": [
        "starttime = 20200629 #change to 20191101, yyyymmdd\n",
        "endtime = 20200630 #change to current date\n",
        "key_pos = np.loadtxt('/content/drive/Shared drives/FNA XN Spring 2020 Project 4/Data/dictionaries/posi_dic.txt', dtype=str).tolist()\n",
        "key_neg = np.loadtxt('/content/drive/Shared drives/FNA XN Spring 2020 Project 4/Data/dictionaries/nega_dic.txt', dtype=str).tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7GmzVvT9TlC"
      },
      "source": [
        "countryreference = {}\n",
        "gdeltref = urlopen('http://data.gdeltproject.org/blog/2018-news-outlets-by-country-may2018-update/MASTER-GDELTDOMAINSBYCOUNTRY-MAY2018.TXT').read().decode('utf-8')\n",
        "\n",
        "for line in gdeltref.split('\\n'):\n",
        "  nline = line.split('\\t')\n",
        "  if len(nline)==3:\n",
        "    countryreference[nline[0]] = nline[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHuNJ5e7_B9z"
      },
      "source": [
        "'''\n",
        "English dataset\n",
        "\n",
        "'''\n",
        "\n",
        "gdeltfiles = urlopen('http://data.gdeltproject.org/gdeltv2/masterfilelist.txt').read().decode('utf-8')\n",
        "eng_filelist = []\n",
        "for line in gdeltfiles.split('\\n'):\n",
        "  nline = line.split(' ')\n",
        "  if len(nline)==3:\n",
        "    if 'gkg' in nline[2]:\n",
        "      if int(nline[2][37:45])>=starttime and int(nline[2][37:45])<endtime:\n",
        "        eng_filelist.append(nline[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7xjMR2h_XmH"
      },
      "source": [
        "'''\n",
        "Translingual dataset\n",
        "\n",
        "'''\n",
        "gdeltfiles = urlopen('http://data.gdeltproject.org/gdeltv2/masterfilelist-translation.txt').read().decode('utf-8')\n",
        "transl_filelist = []\n",
        "for line in gdeltfiles.split('\\n'):\n",
        "  nline = line.split(' ')\n",
        "  if len(nline)==3:\n",
        "    if 'gkg' in nline[2]:\n",
        "      if int(nline[2][37:45])>=starttime and int(nline[2][37:45])<endtime:\n",
        "        transl_filelist.append(nline[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3gVhRndU--7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_KD1jaBCw-C",
        "outputId": "57e5a14e-8fed-4622-cb2c-d452c26916be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Files in each dataset, Translated:',len(transl_filelist), 'English:',len(eng_filelist))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files in each dataset, Translated: 96 English: 96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8jc1W-g_7AJ"
      },
      "source": [
        "'''\n",
        "Set of Covid-19 themes\n",
        "'''\n",
        "covidthemes = ['CORONAVIRUS', 'DISEASE', 'INFECTIOUS','VIRUS']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFHTm33w5b70"
      },
      "source": [
        "def get_urls(countries, translation=False):\n",
        "  if translation:\n",
        "    flist = transl_filelist\n",
        "  else:\n",
        "    flist = eng_filelist\n",
        "  urldictionary = {}\n",
        "  for i in countries:\n",
        "    urldictionary[i]={}\n",
        "  counter = 0\n",
        "  for fname in tqdm(flist):\n",
        "    resp = urlopen(fname)\n",
        "    zipfile = ZipFile(BytesIO(resp.read()))\n",
        "    zipfile.namelist()\n",
        "    fdate = int(fname[37:45])\n",
        "    for cc, ccds in urldictionary.items():\n",
        "      if fdate not in ccds.keys():\n",
        "        urldictionary[cc][fdate] = []\n",
        "    for file in zipfile.namelist():\n",
        "      for line in zipfile.open(file).readlines():\n",
        "          try:\n",
        "            nline = line.decode('utf-8').split('\\t')\n",
        "            dom = nline[3]\n",
        "            url = nline[4]\n",
        "            themes = nline[8]#.split(';')\n",
        "            is_covid = False\n",
        "            for i in covidthemes:\n",
        "              if i in themes:\n",
        "                is_covid = True\n",
        "            if is_covid:\n",
        "              if dom in countryreference.keys():\n",
        "                if countryreference[dom] in countries:\n",
        "                  urldictionary[countryreference[dom]][fdate].append((dom, url))\n",
        "                  counter+=1\n",
        "                  #print(counter)\n",
        "          except:\n",
        "            continue\n",
        "  print('Found', counter, 'articles!')\n",
        "  return urldictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2faov8HTG44B"
      },
      "source": [
        "def cleantext(text):\n",
        "    text = re.sub(r'\\n','',text)\n",
        "    text = re.sub(r'\\t','',text)\n",
        "    text = re.sub(r'\\r','',text)\n",
        "    \n",
        "    text = re.sub(r\"\\’\", \"'\", text)#smart single quotes\n",
        "    text = re.sub(r\"\\“\", '\"', text)#smart double quotes->delete\n",
        "    text = re.sub(r\"\\”\", '\"', text)    \n",
        "    text = re.sub(r\"\\—\", \"-\", text)\n",
        "\n",
        "    \n",
        "    #text = re.sub(r\"\\\"\", \"\", text)\n",
        "    #text = re.sub(r\"\\-\", \" \", text)\n",
        "    text = re.sub(r\"\\xa0\", \" \", text)\n",
        "\n",
        "    #text = re.sub(\"\\S*\\d\\S*\", \"\", text).strip()\n",
        "    text = re.sub('''[^A-Za-z0-9-.!?,;$#@\\(/)'\"\"]+''', ' ', text)###Apostrphe included\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    #text = re.sub(r\"\\'s\", \" is\", text)\n",
        "    return text\n",
        "\n",
        "def scrapetext(url):\n",
        "    #print('text', url)\n",
        "    try:\n",
        "      # open the url using urllib.request and put the HTML into the page variable\n",
        "      page = urlopen(url)\n",
        "      soup = BeautifulSoup(page, \"lxml\")\n",
        "      texts = ''\n",
        "      for p in soup.body.find_all('p'):\n",
        "          if len(p.text)>30:\n",
        "              texts+=' '+p.text\n",
        "      #print(cleantext)\n",
        "      return cleantext(texts)\n",
        "    except:\n",
        "        #print(url, 'not reachable for scraping')\n",
        "        return ''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-7VtS4cNIFF"
      },
      "source": [
        "def body_keyword_count_lemma(body, key_positive, key_negative):\n",
        "    lemmatizer = WordNetLemmatizer() \n",
        "    counter_p = 0 \n",
        "    counter_n = 0\n",
        "    for word in body.split():\n",
        "        # print('Comparing: ', word)\n",
        "        for key_p in key_positive:\n",
        "            if lemmatizer.lemmatize(key_p) in word:\n",
        "                # print('Positive word: ',key_p)\n",
        "                counter_p += 1\n",
        "                break\n",
        "        for key_n in key_negative:\n",
        "            if lemmatizer.lemmatize(key_n) in word:\n",
        "                # print('Negative word: ',key_n)\n",
        "                counter_n += 1\n",
        "                break        \n",
        "        \n",
        "    return (len(body.split()), counter_p, counter_n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pU_50oF8xd0"
      },
      "source": [
        "def processurls(urldict, savename, save=True, threshold=500):\n",
        "  usedlinks = []\n",
        "  for key, links in urldict.items():\n",
        "    dates, doms, urls, texts, total, posit, negat = [],[],[],[],[],[],[]\n",
        "    counter=0\n",
        "    for i in links:\n",
        "      if counter >threshold:\n",
        "        break\n",
        "      if i[1] not in usedlinks:\n",
        "        text = scrapetext(i[1])\n",
        "        if len(text)>50: #50 character limit\n",
        "          counter+=1\n",
        "          tot, pos, neg = body_keyword_count_lemma(text, key_pos, key_neg)\n",
        "          dates.append(key)\n",
        "          doms.append(i[0])\n",
        "          urls.append(i[1])\n",
        "          texts.append(text)\n",
        "          total.append(tot)\n",
        "          posit.append(pos)\n",
        "          negat.append(neg)\n",
        "          usedlinks.append(i[1])\n",
        "    df = pd.DataFrame({'date':dates,'dom':doms,'url':urls,'text':texts,'total':total,'positive':posit,'negative':negat})\n",
        "    df.to_csv(savename+'_'+str(key)+'.csv', sep=',')\n",
        "  print('DONE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjy0i_KH-4HD"
      },
      "source": [
        "def collect_country(countries, translation, filepath, threshold):\n",
        "  print('Collecting GDELT for', countries)\n",
        "  ccdict = get_urls(countries, translation)\n",
        "  print('Gathered', len(ccdict), 'countries')\n",
        "  for cc, urldict in ccdict.items():\n",
        "    count = 0\n",
        "    for dd, ddlinks in urldict:\n",
        "      count+=len(ddlinks)\n",
        "    print(cc, 'N Articles', count)\n",
        "  #print(ccdict)\n",
        "  for cc, urldict in ccdict.items():\n",
        "    processurls(urldict, filepath+cc, True, threshold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAfO_3gYPUcS",
        "outputId": "556ba1ba-6c8a-4bda-c932-8d73c7186d5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "countryset = ['GM', 'SW']\n",
        "threshold = 500 #daily limit for news pieces\n",
        "savefilepath = '/content/drive/Shared drives/FNA XN Spring 2020 Project 4/Data/fixed_data/'\n",
        "collect_country(countryset, True, savefilepath, threshold) # True for translated and False for eng"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/96 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting GDELT for ['GM', 'SW']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 96/96 [01:33<00:00,  1.03it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 3337 articles!\n",
            "Gathered 2 days\n",
            "DONE\n",
            "DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgxaArAGQLat"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}